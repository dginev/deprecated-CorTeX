#!/usr/bin/perl
# /=====================================================================\ #
# |  CorTeX Framework                                                   | #
# | cortex-client -- TaskDB and Gearman broker                          | #
# |=====================================================================| #
# | Part of the LaMaPUn project: https://trac.kwarc.info/lamapun/       | #
# |  Research software, produced as part of work done by:               | #
# |  the KWARC group at Jacobs University                               | #
# | Copyright (c) 2012                                                  | #
# | Released under the GNU Public License                               | #
# |---------------------------------------------------------------------| #
# | Deyan Ginev <d.ginev@jacobs-university.de>                  #_#     | #
# | http://kwarc.info/people/dginev                            (o o)    | #
# \=========================================================ooo==U==ooo=/ #
use strict;
use warnings;
use Encode;
use Data::Dumper;
use JSON::XS qw(decode_json encode_json);
use Archive::Zip qw(:CONSTANTS :ERROR_CODES);

use FindBin;
my ($RealBin_safe,$libdir);
use File::Basename 'dirname';
use File::Spec::Functions qw/catdir catfile/;
BEGIN {
  $FindBin::RealBin =~ /^([^\0]+)\z/; # Valid Unix path TODO: Windows, revisit regexp
  $RealBin_safe = $1;
  die 'Fatal:IO:tainted RealBin was tainted! Failing...'
   unless ($RealBin_safe && (-e catfile($RealBin_safe,'cortex-client'))); 
  $libdir = catdir($RealBin_safe,"..","lib"); }

if (-d $libdir) {
  use lib $libdir;}

use CorTeX::Backend;
use CorTeX::Util::DB_File_Utils qw(db_file_connect db_file_disconnect);
use CorTeX::Util::Data qw(parse_log);
use CorTeX::Import;
use AnyEvent::Gearman;

# CorTeX workhorses have lowest priority, to avoid overloading the machine
setpriority(0, $$, 20);

my ($CORTEX_DB_DIR,@servers) = @ARGV;
$CORTEX_DB_DIR = undef if ($CORTEX_DB_DIR eq 'default');
# Make sure we're terminating when requested to:
$SIG{'INT'} = \&stop_immediate; # Interrupt handler
$SIG{'HUP'} = \&stop_immediate; # Apache Hangup handler
$SIG{'KILL'} = \&stop_immediate; # Just good-old KILL handler
$SIG{'TERM'} = \&stop_immediate; # TERM handler
$SIG{'ALRM'} = \&restart_client;
sub stop_immediate { exit 0; }
sub restart_client {
  print STDERR "Timeout reached, restarting client $$\n";
  exec("$RealBin_safe/cortex-worker",$CORTEX_DB_DIR,@servers)
    or die("Fatal:cortex-client:restart Client $$ autoflush Failed!"); }

# Grab a TaskDB
my $db_handle = db_file_connect($CORTEX_DB_DIR);
$db_handle->{CORTEX_DB_DIR} = $CORTEX_DB_DIR if $CORTEX_DB_DIR;
my $backend = CorTeX::Backend->new(%$db_handle);
my $taskdb = $backend->taskdb;
my $docdb = $backend->docdb;
my $metadb = $backend->metadb;
db_file_disconnect($db_handle);

### MAIN ###
# Event loop between DB Polling and Gearman dispatching

my $max_queue_size = 30;
my $timeout_limit = 600; # Alive for ten minutes, unless a job returns;
my $queue_size;
my $client = gearman_client(@servers);

my $results = [];
my $task_queue_size = 0;

alarm($timeout_limit); # Timeout if we're inactive for too long
while (1) {
  my ($mark,$tasks) = $taskdb->fetch_tasks(size=>$max_queue_size);
  $queue_size = scalar(@$tasks);
  if (! ($queue_size)) {
    # Try again in a minute if queue is empty
    my $sleep_cv = AnyEvent->condvar;
    my $wait_sixty_seconds = AnyEvent->timer(after => 60, cb => sub { $sleep_cv->send });
    $sleep_cv->recv; next; }
  $task_queue_size += $queue_size;
  my $next_iteration = AnyEvent->condvar;
  QUEUE_TASKS:
  foreach my $task(@$tasks) {
    my $entry = $task->{entry};
    my $taskid = $task->{taskid};
    my $service = $task->{iid};
    $task->{formats} = $taskdb->serviceiid_to_formats($service);
    # Obtain the actual workload:
    my $inputconverter = $task->{formats}->[2];
    $inputconverter = $taskdb->serviceid_to_iid(
                      $taskdb->service_to_id($inputconverter))
      if $inputconverter;
    my $entry_setup = $task->{formats}->[3];
    ### SPECIAL CASE corpus import:
    if ($service =~ /^init\_v/) { # Initialize a corpus, do not dispatch to gearman!
      my $dbfile = db_file_connect($ENV{CORTEX_DB_DIR});
      my $upper_bound = $dbfile->{$task->{entry}.'-upper-bound'};
      my $organization = $dbfile->{$task->{entry}.'-organization'};
      # The new method will unpack/reorder any non-canonical organizations
      my $importer = CorTeX::Import->new(
        root=>$task->{entry},
        upper_bound=>$upper_bound,
        organization=>$organization,
        verbosity=>1, 
        %$dbfile);
      db_file_disconnect($dbfile);
      # Now that we are canonical - process all
      while ($importer->process_next) {}
      # Add the message to the TaskDB logs, mark as complete.
      my $response = {service=>$service,entry=>$entry,taskid=>$taskid,
          messages=>parse_log($importer->{log}),status=>-1};
      # Proceed to Next task
      push @$results, $response;
      $task_queue_size--;
      if (scalar(@$results) >= int($queue_size/2)) {
        complete_tasks($results);
        $results=[];
        if ($task_queue_size < $max_queue_size) {
          $next_iteration->send(); } } }
    ### REGULAR CASE: Gearman task
    else {
      # TODO: Obtain prerequisite annotations/resources
      #       And only then encode as JSON
      # For now: obtain entry
      $task->{workload} = $docdb->fetch_entry(
        entry=>$task->{entry},
        inputformat=>$task->{formats}->[0],
        inputconverter=>$inputconverter,
        'entry-setup'=>$entry_setup);
      # Whenever a job fails, we should return a meaningful message for well-organised bookkeeping
      sub fail_handler {
        my $message = $_[1] || "Job failed (generic)";
        my $response = {service=>$service,entry=>$entry,taskid=>$taskid,
            messages=>parse_log("Fatal:Gearman:client $message"),status=>-4};
        push @$results, $response;
        $task_queue_size--;
        # New queue if we're done here
        print STDERR "[".localtime()."] Client $$ : Failed #".scalar(@$results)."\n";
        alarm($timeout_limit); # Reset alarm
        if (scalar(@$results) >= int($queue_size/2)) {
          # Done with this queue, report to DB
          # TODO: Push new annotations, formats, aggregate resources into docdb
          # TODO: Important - try to do so asynchroneously, the Doc DB might become a bottleneck otherwise
          print STDERR "[".localtime()."] Client $$ : Completing tasks ...\n";
          complete_tasks($results);
          $results=[];
          print STDERR "[".localtime()."] Client $$ : Tasks completed!\n";
          if ($task_queue_size < $max_queue_size) {
            $next_iteration->send(); } } }

      # Dispatch to Gearman
      $client->add_task(
        $service => $task->{workload},
        on_complete => sub {
            my $payload = $_[1];
            my $response={};
            if ($entry_setup) {# complex setup
              # We expect $payload to be a ZIP archive with a _cortex_log.txt log, and _cortex_status.txt status.
              my $archive = Archive::Zip->new();
              my $content_handle = IO::String->new($payload);
              if ($archive->readFromFileHandle( $content_handle ) != AZ_OK) {
                fail_handler(undef,'Job failed (invalid archive returned)');
                return; }
              $response->{log} = $archive->contents('_cortex_log.txt');
              $response->{status} = $archive->contents('_cortex_status.txt');
              $archive->removeMember('_cortex_log.txt');
              $archive->removeMember('_cortex_status.txt');
              $response->{document} = $archive; }
            else { # Simple setup is already in JSON
              $response = decode_json($payload); }
            # Parse log, to be ready for TaskDB insertion
            my $messages = parse_log(delete $response->{log});
            @$messages = grep {$_->{severity} ne 'status'} @$messages; # We don't want status messages for now. TODO: What to do with those?
            $response->{messages} = $messages;
            # Record the service and entry names (TODO: also corpus name or? ensure all entries are unique instead?)
            $response->{entry} = $entry;
            $response->{taskid} = $taskid;
            $response->{service} = $service;
            $response->{formats} = $task->{formats};
            push @$results, $response;
            $task_queue_size--;
            # New queue if we're done here
            print STDERR "[".localtime()."] Client $$ : ".scalar(@$results)." jobs completed\n";
            alarm($timeout_limit); # Reset alarm
            if (scalar(@$results) >= int($queue_size/2)) {
              # Done with this queue, report to DB
              # TODO: Push new annotations, formats, aggregate resources into docdb
              # TODO: Important - try to do so asynchroneously, the Doc DB might become a bottleneck otherwise
              complete_tasks($results);
              $results=[];
              if ($task_queue_size < $max_queue_size) {
                $next_iteration->send(); } } },
        on_fail => \&fail_handler
      );
    }
  }
  $next_iteration->recv; }

sub complete_tasks {
  my ($results) = @_;
  # TODO: A single transaction with each database here, speed everything along nicely
  # TODO: Validate the service is returning the correct type of data
  # Insert any new annotations from the conversion
  $metadb->complete_annotations($results);
  # Insert any new documents or resources from the conversion
  $docdb->complete_documents($results);
  # Update all as done, insert logs
  $taskdb->complete_tasks($results); }