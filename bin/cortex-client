#!/usr/bin/env perl
# /=====================================================================\ #
# |  CorTeX Framework                                                   | #
# | cortex-client -- TaskDB and Gearman broker                          | #
# |=====================================================================| #
# | Part of the LaMaPUn project: https://trac.kwarc.info/lamapun/       | #
# |  Research software, produced as part of work done by:               | #
# |  the KWARC group at Jacobs University                               | #
# | Copyright (c) 2012                                                  | #
# | Released under the GNU Public License                               | #
# |---------------------------------------------------------------------| #
# | Deyan Ginev <d.ginev@jacobs-university.de>                  #_#     | #
# | http://kwarc.info/people/dginev                            (o o)    | #
# \=========================================================ooo==U==ooo=/ #
use strict;
use warnings;
use Encode;
use JSON::XS qw(decode_json encode_json);
use Archive::Zip qw(:CONSTANTS :ERROR_CODES);
use List::Util qw(min max sum);
use Scalar::Util qw(weaken);

use FindBin;
my ($RealBin_safe,$libdir);
use File::Basename 'dirname';
use File::Spec::Functions qw(catdir catfile);
BEGIN {
  $FindBin::RealBin =~ /^([^\0]+)\z/; # Valid Unix path TODO: Windows, revisit regexp
  $RealBin_safe = $1;
  die 'Fatal:IO:tainted RealBin was tainted! Failing...'
   unless ($RealBin_safe && (-e catfile($RealBin_safe,'cortex-client'))); 
  $libdir = catdir($RealBin_safe,"..","lib"); }

if (-d $libdir) {
  use lib $libdir;}

use CorTeX::Util::DB_File_Utils qw(db_file_connect db_file_disconnect);
use CorTeX::Util::Data qw(parse_log);
use CorTeX::Util::Gearman qw(available_workers);
use CorTeX::Backend;
use CorTeX::Import;
use AnyEvent::Gearman;

# CorTeX workhorses have lowest priority, to avoid overloading the machine
setpriority(0, $$, 20);

my ($CORTEX_DB_DIR,@servers) = @ARGV;
$CORTEX_DB_DIR = undef if ($CORTEX_DB_DIR eq 'default');
# Make sure we're terminating when requested to:
$SIG{'INT'} = \&stop_immediate; # Interrupt handler
$SIG{'HUP'} = \&stop_immediate; # Apache Hangup handler
$SIG{'KILL'} = \&stop_immediate; # Just good-old KILL handler
$SIG{'TERM'} = \&stop_immediate; # TERM handler
sub stop_immediate { exit 0; }

# Grab a TaskDB
my $db_handle = db_file_connect($CORTEX_DB_DIR);
$db_handle->{CORTEX_DB_DIR} = $CORTEX_DB_DIR if $CORTEX_DB_DIR;
my $backend = CorTeX::Backend->new(%$db_handle);
my $taskdb = $backend->taskdb;
my $docdb = $backend->docdb;
my $metadb = $backend->metadb;
db_file_disconnect($db_handle);


my $results = [];
my $task_queue = {};
my $tasks_processed_counter=0;
my @marks;

my $timeout_limit = 600; # Alive for ten minutes, unless a job returns;
my $queue_size;

our @clients = ();
our $wait_timeout;
our ($main_loop_cv,$sleep_cv,$handler_sleep_cv);
sub wake_sleep_cv {$sleep_cv->send;}
sub wake_handler_sleep_cv {$handler_sleep_cv->send;}

our ($mark,$tasks)=(undef,[]);
our $max_queue_size;
our $client = gearman_client(@servers);

### MAIN ###
# Event loop between DB Polling and Gearman dispatching
while (1) {
  $max_queue_size = compute_max_queue_size();

  # After ten full iterations, we restart cleanly
  restart_client(clean=>1) if ($tasks_processed_counter >= 10 * $max_queue_size);

  @$tasks = (); # Start each iteration with an empty set of tasks
  $main_loop_cv = AnyEvent->condvar; # Timeout if we're inactive for too long
  # Fetch the tasks we are going to dispatch:
  if (scalar(keys %$task_queue) < $max_queue_size) {
    ($mark,$tasks) = $taskdb->fetch_tasks(size=>$max_queue_size,hosts=>\@servers);
    push @marks, $mark;
    $queue_size = scalar(@$tasks); }
  # Prepare timeout:
  $wait_timeout = AnyEvent->timer(after => $timeout_limit, cb => \&restart_client); 
  if (! ($queue_size)) {
    if (scalar(keys %$task_queue)) {
      # There are pending tasks, so just wait for the to send an event...
    }
    else { # Otherwise, the queue is empty and we can wait a minute to avoid busy loops
      print STDERR "[".localtime()."] Client $$ : Queue is empty, sleeping...\n";
      # Try again in a minute if queue is empty
      $sleep_cv = AnyEvent->condvar;
      my $wait_sixty_seconds = AnyEvent->timer(after => 60, cb =>\&wake_sleep_cv);
      $sleep_cv->recv;
      # Move to the next cycle
      $main_loop_cv->send; }}
  else {
  #print STDERR "[".localtime()."] Client $$ : QSize($task_queue_size)\n";
  QUEUE_TASKS:
  foreach my $task(@$tasks) {
    my $entry = $task->{entry};
    my $taskid = $task->{taskid};
    $task_queue->{"$taskid"} = $task;
    my $service = $task->{iid};
    my $description = $taskdb->service_description(iid=>$service);
    # Obtain the actual workload:
    my $inputformat = $description->{inputformat};
    my $inputconverter = $description->{inputconverter};
    $inputconverter = $taskdb->serviceid_to_iid(
                      $taskdb->service_to_id($inputconverter))
      if $inputconverter;
    my $entry_setup = $description->{entrysetup};
    ### SPECIAL CASE corpus import:
    if ($service =~ /^init\_v/) { # Initialize a corpus, do not dispatch to gearman!
      $wait_timeout = undef;
      complete_tasks($results) if @$results;
      my $dbfile = db_file_connect($ENV{CORTEX_DB_DIR});
      my $upper_bound = $dbfile->{$entry.'-upper-bound'};
      my $organization = $dbfile->{$entry.'-organization'};
      # The new method will unpack/reorder any non-canonical organizations
      my $importer = CorTeX::Import->new(
        root=>$entry,
        upper_bound=>$upper_bound,
        organization=>$organization,
        verbosity=>1, 
        %$dbfile);
      db_file_disconnect($dbfile);
      # Now that we are canonical - process all
      $importer->process_all();
      # Add the message to the TaskDB logs, mark as complete.
      my $response = {service=>$service,entry=>$entry,taskid=>$taskid,
          messages=>parse_log($importer->{log}),status=>-1};
      # Proceed to Next task
      push @$results, $response;
      delete $task_queue->{"$taskid"};
      $tasks_processed_counter++;
      if (scalar(@$results) >= int($queue_size/2)) {
        complete_tasks($results) if @$results;
        if (scalar(keys %$task_queue) < $max_queue_size) {
          $main_loop_cv->send(); } } }
    ### REGULAR CASE: Gearman task
    else {
      # TODO: Obtain prerequisite annotations/resources
      #       And only then encode as JSON
      # For now: obtain entry

      # Dispatch to Gearman
      my $gearman_task = $client->add_task(
        $service => $docdb->fetch_entry(
                      entry=>$entry,
                      inputformat=>$inputformat,
                      inputconverter=>$inputconverter,
                      xpath=>$description->{xpath},
                      'entry-setup'=>$entry_setup),
        on_complete => \&complete_handler,
        on_fail => \&fail_handler
      );
      # Black magic HACK -- force in parameters inside the Gearman task
      #  we're brutally violing OO encapsulation here, but 
      #  AnyEvent::Gearman just makes this impossible ... 
      # The alternative of making a new sub for each job = huge memory leaks
      $gearman_task->{entry} = $entry;
      $gearman_task->{service} = $service;
      $gearman_task->{taskid} = $taskid;
      $gearman_task->{description} = $description;
      $gearman_task->{entry_setup} = $entry_setup;
      $gearman_task->{extra_fields_set} = 1;
    }
  }}

  $main_loop_cv->recv;
  $wait_timeout = undef;
}

sub complete_tasks {
  my ($completed_jobs) = @_;
  print STDERR "[".localtime()."] Client $$ : Completing tasks ...\n";
  # TODO: A single transaction with each database here, speed everything along nicely
  # TODO: Validate the service is returning the correct type of data
  # Insert any new annotations from the conversion
  $metadb->complete_annotations($completed_jobs);
  # Insert any new documents or resources from the conversion
  $docdb->complete_documents($completed_jobs);
  # Update all as done, insert logs
  $taskdb->complete_tasks($completed_jobs); 
  # Free memory, if possible
  foreach my $response(@$completed_jobs) {
    weaken($response); }
  @$completed_jobs = ();
  print STDERR "[".localtime()."] Client $$ : Tasks completed!\n"; }
  
# Whenever a job fails, we should return a meaningful message for well-organised bookkeeping
sub fail_handler {
  my ($self, $message) = @_;
  # Don't complete before the extra fields are set (defensive black magic)
  while (! $self->{extra_fields_set}) {
    print STDERR "[".localtime()."] Client $$ : Extra fields not set, waiting for completion...\n";
    $handler_sleep_cv = AnyEvent->condvar;
    my $wait_a_second = AnyEvent->timer(after => 1, cb =>\&wake_handler_sleep_cv);
    $handler_sleep_cv->recv; }
  # Obtain a response message and payload
  $message = "Job failed (generic)" unless $message;
  my $response = {service=>$self->{service},entry=>$self->{entry},taskid=>$self->{taskid},
      messages=>parse_log("Fatal:Gearman:client $message"),status=>-4};
  push @$results, $response;
  $wait_timeout = undef; # Reset the timer
  delete $task_queue->{$self->{taskid}};
  $tasks_processed_counter++;
  # New queue if we're done here
  print STDERR "[".localtime()."] Client $$ : Failed #".scalar(@$results)."\n$message\n";
  if (scalar(@$results) >= int($queue_size/2)) {
    # Done with this queue, report to DB
    # TODO: Push new annotations, formats, aggregate resources into docdb
    # TODO: Important - try to do so asynchroneously, the Doc DB might become a bottleneck otherwise
    complete_tasks($results) if @$results; }
  # Try to free memory
  weaken $self;
  # Continue main loop
  $main_loop_cv->send();
  return; }


sub complete_handler {
  my ($self,$payload) = @_;
  # Don't complete before the extra fields are set (defensive black magic)
  while (! $self->{extra_fields_set}) {
    print STDERR "[".localtime()."] Client $$ : Extra fields not set, waiting for completion...\n";
    $handler_sleep_cv = AnyEvent->condvar;
    my $wait_a_second = AnyEvent->timer(after => 1, cb =>\&wake_handler_sleep_cv);
    $handler_sleep_cv->recv; }
  # Record the service and entry names (TODO: also corpus name or? ensure all entries are unique instead?)
  my $response={
    entry=> $self->{entry},
    taskid=> $self->{taskid},
    service=> $self->{service},
    description=> $self->{description}};
  $wait_timeout = undef; # Reset the timer
  delete $task_queue->{$self->{taskid}};
  $tasks_processed_counter++;
  if ($self->{entry_setup}) {# complex setup
    # We expect $payload to be a ZIP archive with a _cortex_log.txt log, and _cortex_status.txt status.
    my $archive = Archive::Zip->new();
    my $content_handle = IO::String->new($payload);
    if ($archive->readFromFileHandle( $content_handle ) != AZ_OK) {
      fail_handler($self,'Job failed (invalid archive returned)');
      weaken $content_handle;
      return; }
    weaken $content_handle;
    $response->{log} = $archive->contents('_cortex_log.txt');
    $response->{status} = $archive->contents('_cortex_status.txt');
    $archive->removeMember('_cortex_log.txt');
    $archive->removeMember('_cortex_status.txt');
    $response->{document} = $archive; }
  else { # Simple setup is already in JSON
    $response = decode_json($payload); }
  # Parse log, to be ready for TaskDB insertion
  my $messages = parse_log(delete $response->{log});
  @$messages = grep {$_->{severity} ne 'status'} @$messages; # We don't want status messages for now. TODO: What to do with those?
  $response->{messages} = $messages;
  push @$results, $response;
  # New queue if we're done here
  print STDERR "[".localtime()."] Client $$ : ".scalar(@$results)." jobs completed\n";
  if (scalar(@$results) >= int($queue_size/2)) {
    # Done with this queue, report to DB
    complete_tasks($results) if @$results; }
  # Try to free memory 
  weaken $self;
  # Continue main loop
  $main_loop_cv->send();
  return; }

sub restart_client {
  my (%options) = @_;
  if ($options{clean}) {
    # Clean, put the remaining tasks back on the queue
    print STDERR "[".localtime()."] Client $$ : Job limit reached, restarting\n";
    my $sth = $taskdb->prepare("UPDATE tasks SET status=-5 WHERE status=?");
    foreach my $current_mark(@marks) {
      $sth->execute($current_mark); }}
  else {
    # Dirty, possibly a timeout, mark all remaining tasks as fatal!
    print STDERR "[".localtime()."] Client $$ : Timeout reached, restarting\n";
    foreach my $taskid(keys %$task_queue) {
      my $task = $task_queue->{"$taskid"};
      my $response = {service=>$task->{iid},entry=>$task->{entry},taskid=>$taskid,
        messages=>parse_log("Fatal:Gearman:client Client Timeout"),status=>-4};
      push @$results, $response; }
    complete_tasks($results) if @$results; }

  # Perform the restart:
  exec("$RealBin_safe/cortex-client",$CORTEX_DB_DIR,@servers)
    or die("Fatal:cortex-client:restart Client $$ autoflush Failed!"); }

sub compute_max_queue_size {
  my $worker_report = available_workers(\@servers);
  my $max_worker_count = 1;
  # Don't queue more tasks at once than the number of the most represented workers.
  $max_worker_count = max(values(%$worker_report))||1 if ref $worker_report;
  # Cap at 30 tasks in queue, since I'm conserned about locking the DB and running out of RAM.
  # TODO: Maybe that is a silly concern and the cap can be higher? Investigate.
  return min(30,$max_worker_count); }