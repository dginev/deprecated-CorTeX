\documentclass{article}
\usepackage{ids}
\usepackage{planetmath-specials}
\usepackage{pmath}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}

% used for TeXing text within eps files
%\usepackage{psfrag}
% need this for including graphics (\includegraphics)
%\usepackage{graphicx}
% for neatly defining theorems and propositions
%\usepackage{amsthm}
% making logically defined graphics
%\usepackage{xypic} 

% there are many more packages, add them here as you need them

% define commands here
\newcommand{\md}{d}
\newcommand{\mv}[1]{\mathbf{#1}}	% matrix or vector
\newcommand{\mvt}[1]{\mv{#1}^{\mathrm{T}}}
\newcommand{\mvi}[1]{\mv{#1}^{-1}}
\newcommand{\mderiv}[1]{\frac{\md}{\md {#1}}} %d/dx
\newcommand{\mnthderiv}[2]{\frac{\md^{#2}}{\md {#1}^{#2}}} %d^n/dx
\newcommand{\mpderiv}[1]{\frac{\partial}{\partial {#1}}} %partial d^n/dx
\newcommand{\mnthpderiv}[2]{\frac{\partial^{#2}}{\partial {#1}^{#2}}} %partial d^n/dx
\newcommand{\borel}{\mathfrak{B}}
\newcommand{\integers}{\mathbb{Z}}
\newcommand{\rationals}{\mathbb{Q}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\complexes}{\mathbb{C}}
\newcommand{\naturals}{\mathbb{N}}
\newcommand{\defined}{:=}
\newcommand{\var}{\mathrm{var}}
\newcommand{\cov}{\mathrm{cov}}
\newcommand{\corr}{\mathrm{corr}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\powerset}[1]{\mathcal{P}(#1)}
\newcommand{\bra}[1]{\langle#1 \vert}
\newcommand{\ket}[1]{\vert \hspace{1pt}#1\rangle}
\newcommand{\braket}[2]{\langle #1 \ket{#2}}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\norm}[1]{\left|\left|#1\right|\right|}
\newcommand{\esssup}{\mathrm{ess\ sup}}
\newcommand{\Lspace}[1]{L^{#1}}
\newcommand{\Lone}{\Lspace{1}}
\newcommand{\Ltwo}{\Lspace{2}}
\newcommand{\Lp}{\Lspace{p}}
\newcommand{\Lq}{\Lspace{q}}
\newcommand{\Linf}{\Lspace{\infty}}
\begin{document}
\paragraph{Definition (Discrete)}
Let $(\Omega, \mathcal{F}, \mu)$ be a discrete probability space, and let $X$ and $Y$ be discrete random variables on $\Omega$.

The conditional entropy $H[X|Y]$, read as ``the conditional entropy of $X$ given $Y$,'' is defined as
\begin{equation}
H[X|Y] = -\sum_{x \in X}\sum_{y \in Y} \mu(X=x,Y=y) \log \mu(X=x|Y=y)
\end{equation}
where $\mu(X|Y)$ denotes the conditional probability. $\mu(Y=y)$ is nonzero in the
discrete case

\paragraph{Discussion}
The results for discrete conditional entropy will be assumed to hold for the continuous case unless we indicate otherwise.

With $H[X,Y]$ the joint entropy and $f$ a function, we have the following results:
\begin{align}
H[X|Y] + H[Y]&= H[X,Y]&\\
H[X|Y] &\le H[X] \hspace{10mm}\text{(conditioning reduces entropy)}\\
H[X|Y] &\le H[X] + H[Y] \hspace{10mm}\text{(equality iff } X, Y \text{ independent)}\\
H[X|Y] &\le H[X|f(Y)]&\\
H[X|Y] &= 0 \iff X=f(Y)\hspace{10mm}\text{(special case } H[X|X] = 0 \text{)}\\
\end{align}

The conditional entropy $H[X|Y]$ may be interpreted as the uncertainty in $X$ given knowledge of $Y$.  (Try reading the above equalities and inequalities with this interpretation in mind.)

%%Feb 2006 removed previos symmetric condition (5). Seems to only hold when %%%%%probabity dist is symmetric.
%%H[X|Y] &= H[Y|X] &\text{(symmetry)}\\
\end{document}
