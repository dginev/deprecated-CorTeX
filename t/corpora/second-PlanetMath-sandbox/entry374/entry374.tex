\documentclass{article}
\usepackage{planetmath-specials}
\usepackage{pmath}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}

% used for TeXing text within eps files
\usepackage{psfrag}
% need this for including graphics (\includegraphics)
\usepackage{graphicx}
% for neatly defining theorems and propositions
%\usepackage{amsthm}
% making logically defined graphics
%\usepackage{xypic} 

% there are many more packages, add them here as you need them

% define commands here
\newcommand{\md}{d}
\newcommand{\mv}[1]{\mathbf{#1}}	% matrix or vector
\newcommand{\mvt}[1]{\mv{#1}^{\mathrm{T}}}
\newcommand{\mvi}[1]{\mv{#1}^{-1}}
\newcommand{\mderiv}[1]{\frac{\md}{\md {#1}}} %d/dx
\newcommand{\mnthderiv}[2]{\frac{\md^{#2}}{\md {#1}^{#2}}} %d^n/dx
\newcommand{\mpderiv}[1]{\frac{\partial}{\partial {#1}}} %partial d^n/dx
\newcommand{\mnthpderiv}[2]{\frac{\partial^{#2}}{\partial {#1}^{#2}}} %partial d^n/dx
\newcommand{\borel}{\mathfrak{B}}
\newcommand{\integers}{\mathbb{Z}}
\newcommand{\rationals}{\mathbb{Q}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\complexes}{\mathbb{C}}
\newcommand{\naturals}{\mathbb{N}}
\newcommand{\defined}{:=}
\newcommand{\var}{\mathrm{var}}
\newcommand{\cov}{\mathrm{cov}}
\newcommand{\corr}{\mathrm{corr}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\powerset}[1]{\mathcal{P}(#1)}
\newcommand{\bra}[1]{\langle#1 \vert}
\newcommand{\ket}[1]{\vert \hspace{1pt}#1\rangle}
\newcommand{\braket}[2]{\langle #1 \ket{#2}}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\norm}[1]{\left|\left|#1\right|\right|}
\newcommand{\esssup}{\mathrm{ess\ sup}}
\newcommand{\Lspace}[1]{L^{#1}}
\newcommand{\Lone}{\Lspace{1}}
\newcommand{\Ltwo}{\Lspace{2}}
\newcommand{\Lp}{\Lspace{p}}
\newcommand{\Lq}{\Lspace{q}}
\newcommand{\Linf}{\Lspace{\infty}}
\newcommand{\sequence}[1]{\{#1\}}
\begin{document}
\PMlinkescapeword{satisfy}
\PMlinkescapeword{satisfies}
\PMlinkescapeword{property}
\PMlinkescapeword{properties}
\PMlinkescapeword{unit}
\PMlinkescapeword{group}
\PMlinkescapeword{groups}
\PMlinkescapeword{measure}
\PMlinkescapeword{uniform}
\PMlinkescapeword{outcome}
\PMlinkescapeword{outcomes}
\PMlinkescapeword{information}
\PMlinkescapeword{information content}
\PMlinkescapeword{symmetric}
\PMlinkescapeword{partition}
\PMlinkescapeword{contains}



\paragraph{Definition (Discrete)}

Let $X$ be a discrete random variable on a finite set
$\mathcal{X}=\{x_1,\ldots,x_n\}$, with probability distribution
function $p(x) = \Pr(X=x)$. The {\em entropy} $H(X)$ of $X$ is
defined as
\begin{equation}
H(X) = -\sum_{x \in \mathcal{X}} p(x) \log_b p(x).
\end{equation}
The convention $0 \log 0 =0$ is adopted in the definition. The
logarithm is usually taken to the base 2, in which case the
entropy is measured in ``bits,'' or to the base {\em e}, in which case
$H(X)$ is measured in ``nats.''

If $X$ and $Y$ are random variables on $\mathcal{X}$ and
$\mathcal{Y}$ respectively, the {\em joint entropy} of $X$ and $Y$
is
\[
  H(X,Y) = -\sum_{(x,y)\in \mathcal{X}\times \mathcal{Y}} p(x,y)
  \log_b p(x,y),
\]
where $p(x,y)$ denote the joint distribution of $X$ and $Y$.

\paragraph{Discussion}
The Shannon entropy was first introduced by Shannon in 1948 in his
landmark paper ``A Mathematical Theory of Communication.'' The
entropy is a functional of the probability distribution function
$p(x)$, and is sometime written as
\[
 H(p(x_1), p(x_2),\ldots, p(x_n)).
\]
It is noted that the entropy of $X$ does not depend on the actual
values of $X$, it only depends on $p(x)$. The definition of
Shannon's entropy can be written as an expectation
\[
  H(X) = -E[\log_b p(X)].
\]
The quantity $-\log_b p(x)$ is interpreted as the information
content of the outcome $x\in\mathcal{X}$, and is also called the Hartley
information of $x$. Hence the Shannon's entropy is the average
amount of information contained in random variable $X$, it is also
the uncertainty removed after the actual outcome of $X$ is
revealed.


\paragraph{Characterization}

We write $H(X)$ as $H_n(p_1,\ldots,p_n)$. The Shannon entropy
satisfies the following properties.

\begin{enumerate}
\item For any $n$, $H_n(p_1,\ldots,p_n)$ is a continuous and
symmetric function on variables $p_1$, $p_2,\ldots, p_n$.

\item Event of probability zero does not contribute to the entropy, i.e.\
for any $n$,
\[
 H_{n+1}(p_1,\ldots,p_n,0) = H_n(p_1,\ldots,p_n).
\]

\item Entropy is maximized when the probability distribution is
uniform. For all $n$,
\[
H_n(p_1,\ldots,p_n) \leq H_n\Big(\frac{1}{n},\ldots,\frac{1}{n}
\Big).
\]
This follows from Jensen inequality,
\[
   H(X) = E\Big[\log_b \Big( \frac{1}{p(X)}\Big) \Big] \leq
   \log_b \Big( E\Big[ \frac{1}{p(X)} \Big] \Big) = \log_b(n).
\]


\item If $p_{ij}$, $1\leq i \leq m$, $1\leq j \leq n$ are
non-negative real numbers summing up to one, and $q_i =
\sum_{j=1}^n p_{ij}$, then
\[
 H_{mn}(p_{11},\ldots, p_{mn}) = H_m(q_1,\ldots,q_m) +
 \sum_{i=1}^m q_i H_n\Big(\frac{p_{i1}}{q_i},\ldots, \frac{p_{in}}{q_i}
 \Big).
\]
If we partition the $mn$ outcomes of the random experiment into
$m$ groups, each group contains $n$ elements, we can do the
experiment in two steps: first determine the group to which the
actual outcome belongs to, and second find the outcome in this
group. The probability that you will observe group $i$ is $q_i$.
The conditional probability distribution function given group $i$
is $(p_{i1}/q_i,\ldots,p_{in}/q_i)$. The entropy
\[
 H_n\Big(\frac{p_{i1}}{q_i},\ldots, \frac{p_{in}}{q_i} \Big)
\]
is the entropy of the probability distribution conditioned on
group $i$. Property 4 says that the total information is the sum
of the information you gain in the first step, $H_m(q_1,\ldots,
q_m)$, and a weighted sum of the entropies conditioned on each
group.
\end{enumerate}


Khinchin in 1957 showed that the only function satisfying the
above assumptions is of the form:
\begin{displaymath}
H_n(p_1,\ldots,p_n) = -k \sum_{i=1}^n p_i \log p_i
\end{displaymath}
where $k$ is a positive constant, essentially a choice of unit of
measure.



\paragraph{Definition (Continuous)}
Entropy in the continuous case is called \emph{\PMlinkname{differential entropy}{DifferentialEntropy}}.


\paragraph{Discussion---Continuous Entropy}
Despite its seductively analogous form, continuous entropy cannot be obtained as a limiting case of discrete entropy.

We wish to obtain a generally finite measure as the ``bin size'' goes to zero.  In the discrete case, the bin size is the (implicit) width of each of the $n$ (finite or infinite) bins/buckets/states whose probabilities are the $p_n$.  As we generalize to the continuous domain, we must make this width explicit.

To do this, start with a continuous function $f$ discretized as shown in the figure:
\newline
\begin{figure}[!hbp]
\begin{center}
\caption{Discretizing the function $f$ into bins of width $\Delta$}
\psfrag{x_i}{$x_i$}
\psfrag{f(x)}{$f(x)$}
\psfrag{x}{$x$}
\psfrag{delta}{$\Delta$}
\includegraphics[width=\textwidth]{function-with-bins.eps}
\end{center}
\end{figure}
As the figure indicates, by the mean-value theorem there exists a value $x_i$ in each bin such that 
\begin{equation}
f(x_i) \Delta = \int_{i\Delta}^{(i+1)\Delta} f(x) dx
\end{equation}
and thus the integral of the function $f$ can be approximated (in the Riemannian sense) by
\begin{equation}
\int_{-\infty}^{\infty} f(x) dx = \lim_{\Delta \to 0} \sum_{i = -\infty}^{\infty} f(x_i) \Delta
\end{equation}
where this limit and ``bin size goes to zero'' are equivalent.

We will denote
\begin{equation}
H^{\Delta} \defined - \sum_{i=-\infty}^{\infty} \Delta f(x_i) \log \Delta f(x_i)
\end{equation}
and expanding the $\log$ we have
\begin{align}
H^{\Delta} &= - \sum_{i=-\infty}^{\infty} \Delta f(x_i) \log \Delta f(x_i)\\
&= - \sum_{i=-\infty}^{\infty} \Delta f(x_i) \log f(x_i) -\sum_{i=-\infty}^{\infty} f(x_i) \Delta \log \Delta.
\end{align}
As $\Delta \to 0$, we have
\begin{align}
\sum_{i=-\infty}^{\infty} f(x_i) \Delta &\to \int f(x) dx = 1\qquad \text{and}\\
\sum_{i=-\infty}^{\infty} \Delta f(x_i) \log f(x_i) &\to \int f(x) \log f(x) dx
\end{align}
This leads us to our definition of the differential entropy (continuous entropy):
\begin{equation}
h[f] = \lim_{\Delta \to 0} \left[H^{\Delta} + \log \Delta\right]  = -\int_{-\infty}^{\infty} f(x) \log f(x) dx.
\end{equation}
\end{document}
